{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FLAN-T5 Full Fine-Tuning on BioLaySumm Dataset\n",
        "\n",
        "**Author:** Nathan Chung  \n",
        "**Course:** COMP3710 Pattern Analysis  \n",
        "**Task:** Expert-to-Layperson Radiology Report Translation  \n",
        "**Model:** T5-small Full Fine-Tuning (60M parameters)\n",
        "\n",
        "This notebook implements full fine-tuning of T5-small on the BioLaySumm dataset for translating expert radiology reports into layperson-friendly language.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install required packages and set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers datasets accelerate evaluate peft rouge-score\n",
        "%pip install pyyaml tqdm\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# HuggingFace libraries\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    GenerationConfig\n",
        ")\n",
        "from datasets import Dataset, load_dataset\n",
        "import evaluate\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set up configuration for full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for full fine-tuning\n",
        "config = {\n",
        "    # Dataset Configuration\n",
        "    'dataset': {\n",
        "        'name': 'BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track',\n",
        "        'max_source_length': 256,\n",
        "        'max_target_length': 128,\n",
        "        'seed': 42\n",
        "    },\n",
        "    \n",
        "    # Model Configuration\n",
        "    'model': {\n",
        "        'name': 't5-small',  # T5-small for full fine-tuning\n",
        "        'torch_dtype': 'bfloat16'\n",
        "    },\n",
        "    \n",
        "    # Training Configuration\n",
        "    'training': {\n",
        "        'strategy': 'full',\n",
        "        'batch_size': 4,\n",
        "        'gradient_accumulation_steps': 4,  # Effective batch size = 16\n",
        "        'learning_rate': 5e-5,  # Lower LR for full fine-tuning\n",
        "        'num_epochs': 3,\n",
        "        'warmup_steps': 500,\n",
        "        'weight_decay': 0.01,\n",
        "        'max_grad_norm': 1.0,\n",
        "        'eval_steps': 1000,\n",
        "        'save_steps': 1000,\n",
        "        'logging_steps': 100,\n",
        "        'eval_strategy': 'steps',\n",
        "        'save_strategy': 'steps',\n",
        "        'load_best_model_at_end': True,\n",
        "        'report_to': 'none',\n",
        "        'seed': 42\n",
        "    },\n",
        "    \n",
        "    # Output Configuration\n",
        "    'output': {\n",
        "        'root': '/content/outputs',\n",
        "        'run_name': 't5-small-full-finetuning'\n",
        "    },\n",
        "    \n",
        "    # Evaluation Configuration\n",
        "    'evaluation': {\n",
        "        'max_new_tokens': 128,\n",
        "        'num_beams': 4,\n",
        "        'length_penalty': 0.6,\n",
        "        'no_repeat_ngram_size': 3,\n",
        "        'early_stopping': True\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"✅ Configuration set up successfully!\")\n",
        "print(f\"Model: {config['model']['name']}\")\n",
        "print(f\"Strategy: {config['training']['strategy']}\")\n",
        "print(f\"Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"Learning rate: {config['training']['learning_rate']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Loading and Model Setup\n",
        "\n",
        "Load the BioLaySumm dataset and T5-small model for full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset and model\n",
        "print(\"Loading BioLaySumm dataset...\")\n",
        "dataset = load_dataset(config['dataset']['name'], trust_remote_code=False)\n",
        "\n",
        "print(f\"Dataset loaded! Train: {len(dataset['train']):,}, Val: {len(dataset['validation']):,}, Test: {len(dataset['test']):,}\")\n",
        "\n",
        "# Check dataset columns\n",
        "print(\"\\nDataset columns:\")\n",
        "print(f\"Train columns: {dataset['train'].column_names}\")\n",
        "print(f\"Sample data:\")\n",
        "sample = dataset['train'][0]\n",
        "for key, value in sample.items():\n",
        "    print(f\"  {key}: {str(value)[:100]}...\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = config['model']['name']\n",
        "print(f\"\\nLoading {model_name} model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"✅ Model loaded!\")\n",
        "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
        "print(f\"Trainable percentage: {(trainable_params/total_params)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Processing\n",
        "\n",
        "Apply expert-to-layperson prompts and tokenize the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart function to detect correct column names and apply prompts\n",
        "def apply_prompts(examples):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    \n",
        "    # Auto-detect column names\n",
        "    expert_col = None\n",
        "    layman_col = None\n",
        "    \n",
        "    # Try different possible column names\n",
        "    possible_expert_cols = ['expert_report', 'radiology_report', 'medical_report', 'report', 'source']\n",
        "    possible_layman_cols = ['layman_report', 'layman_summary', 'summary', 'lay_summary', 'target']\n",
        "    \n",
        "    for col in possible_expert_cols:\n",
        "        if col in examples:\n",
        "            expert_col = col\n",
        "            break\n",
        "    \n",
        "    for col in possible_layman_cols:\n",
        "        if col in examples:\n",
        "            layman_col = col\n",
        "            break\n",
        "    \n",
        "    if expert_col is None or layman_col is None:\n",
        "        print(f\"Available columns: {list(examples.keys())}\")\n",
        "        raise ValueError(f\"Could not find expert column from {possible_expert_cols} or layman column from {possible_layman_cols}\")\n",
        "    \n",
        "    print(f\"Using columns: expert='{expert_col}', layman='{layman_col}'\")\n",
        "    \n",
        "    for expert_report, layman_report in zip(examples[expert_col], examples[layman_col]):\n",
        "        prompt = f\"Translate this medical report into simple, easy-to-understand language for patients:\\\\n\\\\n{expert_report}\"\n",
        "        input_texts.append(prompt)\n",
        "        target_texts.append(layman_report)\n",
        "    \n",
        "    return {'input_text': input_texts, 'target_text': target_texts}\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    max_source_length = config['dataset']['max_source_length']\n",
        "    max_target_length = config['dataset']['max_target_length']\n",
        "    \n",
        "    model_inputs = tokenizer(\n",
        "        examples['input_text'],\n",
        "        max_length=max_source_length,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "    \n",
        "    labels = tokenizer(\n",
        "        examples['target_text'],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "    \n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process datasets\n",
        "print(\"Processing datasets...\")\n",
        "train_dataset = dataset['train'].map(apply_prompts, batched=True, remove_columns=dataset['train'].column_names)\n",
        "val_dataset = dataset['validation'].map(apply_prompts, batched=True, remove_columns=dataset['validation'].column_names)\n",
        "test_dataset = dataset['test'].map(apply_prompts, batched=True, remove_columns=dataset['test'].column_names)\n",
        "\n",
        "# Tokenize\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "print(f\"✅ Datasets processed!\")\n",
        "print(f\"Tokenized train: {len(tokenized_train):,}\")\n",
        "print(f\"Tokenized validation: {len(tokenized_val):,}\")\n",
        "\n",
        "# Show processed sample\n",
        "print(\"\\nProcessed sample:\")\n",
        "sample = tokenized_train[0]\n",
        "print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
        "print(f\"Labels length: {len(sample['labels'])}\")\n",
        "print(f\"Sample input: {tokenizer.decode(sample['input_ids'][:50])}...\")\n",
        "print(f\"Sample target: {tokenizer.decode(sample['labels'][:30])}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup and Execution\n",
        "\n",
        "Set up training arguments and start training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(config['output']['root']) / config['output']['run_name']\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(output_dir),\n",
        "    num_train_epochs=config['training']['num_epochs'],\n",
        "    per_device_train_batch_size=config['training']['batch_size'],\n",
        "    per_device_eval_batch_size=config['training']['batch_size'],\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "    learning_rate=config['training']['learning_rate'],\n",
        "    weight_decay=config['training']['weight_decay'],\n",
        "    max_grad_norm=config['training']['max_grad_norm'],\n",
        "    warmup_steps=config['training']['warmup_steps'],\n",
        "    eval_strategy=config['training']['eval_strategy'],\n",
        "    eval_steps=config['training']['eval_steps'],\n",
        "    save_strategy=config['training']['save_strategy'],\n",
        "    save_steps=config['training']['save_steps'],\n",
        "    load_best_model_at_end=config['training']['load_best_model_at_end'],\n",
        "    logging_steps=config['training']['logging_steps'],\n",
        "    report_to=config['training']['report_to'],\n",
        "    seed=config['training']['seed'],\n",
        "    bf16=True,\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "\n",
        "print(\"✅ Training setup complete!\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Output directory: {training_args.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"🚀 Starting full fine-tuning training...\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Strategy: Full fine-tuning (100% parameters trainable)\")\n",
        "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
        "\n",
        "# Train the model\n",
        "train_results = trainer.train()\n",
        "\n",
        "print(\"\\n✅ Training completed successfully!\")\n",
        "print(f\"Final train loss: {train_results.training_loss:.4f}\")\n",
        "print(f\"Training time: {train_results.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training samples per second: {train_results.metrics['train_samples_per_second']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation and Sample Predictions\n",
        "\n",
        "Evaluate the trained model on the test set and generate sample predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"🔍 Evaluating model on test set...\")\n",
        "\n",
        "# Tokenize test set\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
        "trainer.eval_dataset = tokenized_test\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n📊 Test Set Evaluation Results:\")\n",
        "print(\"=\" * 50)\n",
        "for metric, value in eval_results.items():\n",
        "    if 'rouge' in metric:\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate sample predictions\n",
        "print(\"\\n🎯 Sample Predictions:\")\n",
        "test_samples = tokenized_test.select(range(3))\n",
        "predictions = trainer.predict(test_samples)\n",
        "\n",
        "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
        "\n",
        "for i in range(len(decoded_preds)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Prediction: {decoded_preds[i]}\")\n",
        "    print(f\"Reference:  {decoded_labels[i]}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Results\n",
        "\n",
        "Save the trained model and results for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "print(\"💾 Saving trained model...\")\n",
        "\n",
        "model_save_path = output_dir / \"final_model\"\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save configuration and results\n",
        "with open(model_save_path / \"training_config.json\", 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "with open(model_save_path / \"evaluation_results.json\", 'w') as f:\n",
        "    json.dump(eval_results, f, indent=2)\n",
        "\n",
        "print(f\"✅ Model saved to: {model_save_path}\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n🎉 Training Complete!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Strategy: Full fine-tuning\")\n",
        "print(f\"Parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"Trainable: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
        "print(f\"ROUGE-1: {eval_results.get('eval_rouge1', 'N/A'):.4f}\")\n",
        "print(f\"ROUGE-2: {eval_results.get('eval_rouge2', 'N/A'):.4f}\")\n",
        "print(f\"ROUGE-L: {eval_results.get('eval_rougeL', 'N/A'):.4f}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
