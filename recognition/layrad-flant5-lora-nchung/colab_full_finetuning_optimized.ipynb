{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T5-small Full Fine-Tuning on BioLaySumm Dataset (Memory Optimized)\n",
        "\n",
        "**Author:** Nathan Chung  \n",
        "**Course:** COMP3710 Pattern Analysis  \n",
        "**Task:** Expert-to-Layperson Radiology Report Translation  \n",
        "**Model:** T5-small Full Fine-Tuning (60M parameters)\n",
        "**Optimized for:** Google Colab T4 GPU (15GB memory limit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers datasets accelerate evaluate rouge-score peft\n",
        "\n",
        "# Mount Google Drive (optional, for backup)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Google Drive not available - continuing without backup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    GenerationConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "\n",
        "# Set environment variables for memory optimization\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration optimized for Colab T4 (15GB memory)\n",
        "config = {\n",
        "    # Model configuration\n",
        "    'model_name': 't5-small',\n",
        "    'task': 'expert_to_layman',\n",
        "    \n",
        "    # Dataset configuration\n",
        "    'dataset_name': 'BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track',\n",
        "    'max_source_length': 256,  # Reduced for memory\n",
        "    'max_target_length': 128,  # Reduced for memory\n",
        "    'max_samples': 10000,      # Limit dataset size for faster training\n",
        "    \n",
        "    # Training configuration (memory optimized)\n",
        "    'batch_size': 1,                    # Minimal batch size\n",
        "    'gradient_accumulation_steps': 16,  # Increase to maintain effective batch size\n",
        "    'learning_rate': 5e-4,             # Slightly higher for faster convergence\n",
        "    'num_epochs': 2,                   # Reduced epochs for faster completion\n",
        "    'warmup_steps': 100,               # Reduced warmup\n",
        "    'weight_decay': 0.01,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'eval_steps': 500,                 # Evaluate every 500 steps\n",
        "    'save_steps': 1000,                # Save every 1000 steps\n",
        "    'logging_steps': 100,              # Log every 100 steps\n",
        "    'seed': 42,\n",
        "    \n",
        "    # Output configuration\n",
        "    'output_dir': '/content/t5-small-full-finetuning',\n",
        "    'run_name': 't5-small-biolaysumm-colab'\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"üìä Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
        "print(f\"üìè Max source length: {config['max_source_length']}\")\n",
        "print(f\"üìè Max target length: {config['max_target_length']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(\"üì• Loading BioLaySumm dataset...\")\n",
        "dataset = load_dataset(config['dataset_name'], split='train')\n",
        "\n",
        "# Limit dataset size for faster training\n",
        "if config['max_samples'] and len(dataset) > config['max_samples']:\n",
        "    dataset = dataset.select(range(config['max_samples']))\n",
        "    print(f\"üìä Limited dataset to {len(dataset)} samples\")\n",
        "\n",
        "# Split into train/validation\n",
        "split_dataset = dataset.train_test_split(test_size=0.1, seed=config['seed'])\n",
        "train_dataset = split_dataset['train']\n",
        "val_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {len(train_dataset)} train, {len(val_dataset)} validation samples\")\n",
        "print(f\"üìã Sample columns: {train_dataset.column_names}\")\n",
        "\n",
        "# Show sample data\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nüìù Sample data:\")\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, str) and len(value) > 100:\n",
        "        print(f\"{key}: {value[:100]}...\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart prompt application function\n",
        "def apply_prompts(examples):\n",
        "    \"\"\"\n",
        "    Apply prompts to dataset examples, auto-detecting column names.\n",
        "    \"\"\"\n",
        "    # Auto-detect column names\n",
        "    expert_cols = ['expert_report', 'radiology_report', 'expert_summary']\n",
        "    layman_cols = ['layman_report', 'layman_summary', 'layperson_summary']\n",
        "    \n",
        "    expert_col = None\n",
        "    layman_col = None\n",
        "    \n",
        "    for col in expert_cols:\n",
        "        if col in examples:\n",
        "            expert_col = col\n",
        "            break\n",
        "    \n",
        "    for col in layman_cols:\n",
        "        if col in examples:\n",
        "            layman_col = col\n",
        "            break\n",
        "    \n",
        "    if not expert_col or not layman_col:\n",
        "        raise ValueError(f\"Could not find expert/layman columns. Available: {list(examples.keys())}\")\n",
        "    \n",
        "    # Apply prompts\n",
        "    if config['task'] == 'expert_to_layman':\n",
        "        input_text = f\"Translate this medical report to layman terms: {examples[expert_col]}\"\n",
        "        target_text = examples[layman_col]\n",
        "    else:\n",
        "        input_text = examples[expert_col]\n",
        "        target_text = examples[layman_col]\n",
        "    \n",
        "    return {\n",
        "        'input_text': input_text,\n",
        "        'target_text': target_text\n",
        "    }\n",
        "\n",
        "# Apply prompts to datasets\n",
        "print(\"üîÑ Applying prompts to datasets...\")\n",
        "train_dataset = train_dataset.map(apply_prompts, remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(apply_prompts, remove_columns=val_dataset.column_names)\n",
        "\n",
        "print(\"‚úÖ Prompts applied successfully\")\n",
        "print(f\"üìù Sample input: {train_dataset[0]['input_text'][:100]}...\")\n",
        "print(f\"üìù Sample target: {train_dataset[0]['target_text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model and Tokenizer Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "print(f\"ü§ñ Loading {config['model_name']} model and tokenizer...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    config['model_name'],\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
        "    device_map='auto'            # Automatic device mapping\n",
        ")\n",
        "\n",
        "# Print model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully\")\n",
        "print(f\"üìä Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
        "print(f\"üíæ Model dtype: {model.dtype}\")\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenization function\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize input and target text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        examples['input_text'],\n",
        "        max_length=config['max_source_length'],\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "    \n",
        "    targets = tokenizer(\n",
        "        examples['target_text'],\n",
        "        max_length=config['max_target_length'],\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "    \n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"üîÑ Tokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    num_proc=1,  # Disable multiprocessing for memory\n",
        "    desc=\"Tokenizing training dataset\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    num_proc=1,  # Disable multiprocessing for memory\n",
        "    desc=\"Tokenizing validation dataset\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Datasets tokenized successfully\")\n",
        "print(f\"üìä Train samples: {len(tokenized_train)}\")\n",
        "print(f\"üìä Validation samples: {len(tokenized_val)}\")\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# ROUGE metrics computation\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Compute ROUGE metrics for evaluation.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "    \n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Compute ROUGE scores\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    \n",
        "    # Extract scores\n",
        "    return {\n",
        "        'rouge1': result['rouge1'],\n",
        "        'rouge2': result['rouge2'],\n",
        "        'rougeL': result['rougeL'],\n",
        "        'rougeLsum': result['rougeLsum']\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Training components prepared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(config['output_dir'])\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training arguments with aggressive memory optimizations\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(output_dir),\n",
        "    num_train_epochs=config['num_epochs'],\n",
        "    per_device_train_batch_size=config['batch_size'],\n",
        "    per_device_eval_batch_size=config['batch_size'],\n",
        "    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
        "    learning_rate=config['learning_rate'],\n",
        "    weight_decay=config['weight_decay'],\n",
        "    max_grad_norm=config['max_grad_norm'],\n",
        "    warmup_steps=config['warmup_steps'],\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=config['eval_steps'],\n",
        "    save_strategy='steps',\n",
        "    save_steps=config['save_steps'],\n",
        "    load_best_model_at_end=False,  # Disable to save memory\n",
        "    logging_steps=config['logging_steps'],\n",
        "    report_to=[],  # No external logging\n",
        "    seed=config['seed'],\n",
        "    bf16=True,                    # Use bfloat16\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,          # Keep fewer checkpoints\n",
        "    # Aggressive memory optimizations\n",
        "    gradient_checkpointing=True,     # Enable gradient checkpointing\n",
        "    dataloader_num_workers=0,        # Disable multiprocessing\n",
        "    dataloader_pin_memory=False,     # Disable pin memory\n",
        "    dataloader_drop_last=True,       # Drop last incomplete batch\n",
        "    prediction_loss_only=False,\n",
        "    include_inputs_for_metrics=True,\n",
        "    eval_accumulation_steps=1,       # Process eval in smaller chunks\n",
        "    fp16=False,                     # Use bf16 instead\n",
        "    tf32=False,                     # Disable TF32\n",
        "    dataloader_disable_tqdm=True    # Disable progress bars\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training setup complete!\")\n",
        "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"üíæ Output directory: {training_args.output_dir}\")\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing checkpoints\n",
        "existing_checkpoints = list(output_dir.glob(\"checkpoint-*\"))\n",
        "\n",
        "if existing_checkpoints:\n",
        "    latest_checkpoint = max(existing_checkpoints, key=lambda x: int(x.name.split('-')[1]))\n",
        "    print(f\"üîÑ Found existing checkpoint: {latest_checkpoint.name}\")\n",
        "    resume_from_checkpoint = str(latest_checkpoint)\n",
        "else:\n",
        "    print(\"üöÄ Starting fresh training...\")\n",
        "    resume_from_checkpoint = None\n",
        "\n",
        "print(f\"ü§ñ Model: {config['model_name']}\")\n",
        "print(f\"üìä Strategy: Full fine-tuning (100% parameters trainable)\")\n",
        "print(f\"üìä Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "print(f\"üìä Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"üèãÔ∏è Starting training...\")\n",
        "\n",
        "try:\n",
        "    train_results = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "    print(f\"üìä Final training loss: {train_results.training_loss:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed: {e}\")\n",
        "    print(\"üí° Try reducing batch_size or max_source_length in config\")\n",
        "    raise\n",
        "finally:\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation and Sample Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run final evaluation\n",
        "print(\"üîç Running final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(\"=\" * 50)\n",
        "for metric, value in eval_results.items():\n",
        "    if 'rouge' in metric:\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate sample predictions\n",
        "print(\"\\nüéØ Sample Predictions:\")\n",
        "test_samples = tokenized_val.select(range(3))\n",
        "predictions = trainer.predict(test_samples)\n",
        "\n",
        "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
        "\n",
        "for i in range(len(decoded_preds)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Prediction: {decoded_preds[i]}\")\n",
        "    print(f\"Reference:  {decoded_labels[i]}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "print(\"üíæ Saving trained model...\")\n",
        "model_save_path = output_dir / \"final_model\"\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'config': config,\n",
        "    'training_results': {\n",
        "        'training_loss': train_results.training_loss,\n",
        "        'training_time': train_results.metrics['train_runtime'],\n",
        "        'samples_per_second': train_results.metrics['train_samples_per_second']\n",
        "    },\n",
        "    'evaluation_results': eval_results\n",
        "}\n",
        "\n",
        "with open(model_save_path / \"results.json\", 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Model and results saved to: {model_save_path}\")\n",
        "\n",
        "# Backup to Google Drive (if available)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if drive.is_mounted():\n",
        "        drive_backup_path = \"/content/drive/MyDrive/Colab Notebooks/t5-small-full-finetuning\"\n",
        "        print(f\"üì§ Backing up to Google Drive...\")\n",
        "        shutil.copytree(output_dir, drive_backup_path, dirs_exist_ok=True)\n",
        "        print(\"‚úÖ Backup to Google Drive completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Google Drive backup failed: {e}\")\n",
        "\n",
        "print(\"\\nüéâ All done! Training completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
