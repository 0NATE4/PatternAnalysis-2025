# FLAN-T5 Base LoRA Test Configuration
# Quick 15-minute test run for validation
# Author: Nathan Chung
# Course: COMP3710 Pattern Analysis

# Dataset Configuration
dataset:
  name: "BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track"
  max_source_length: 256      # Shorter for faster test
  max_target_length: 128      # Shorter for faster test
  seed: 42                    # Random seed for reproducible shuffling
  local_data_path: null       # Optional local data path override

# Model Configuration
model:
  name: "google/flan-t5-base"  # Base FLAN-T5 model
  torch_dtype: "bfloat16"      # Mixed precision for memory efficiency

# Training Configuration
training:
  strategy: "lora"            # Training strategy: 'lora' or 'full'
  batch_size: 4               # Small batch for test
  gradient_accumulation_steps: 2  # Effective batch size = 4 * 2 = 8
  learning_rate: 1e-4         # Learning rate for LoRA
  num_epochs: 1               # Just 1 epoch for test
  max_steps: 25               # Very few steps for quick test
  warmup_steps: 5             # Small warmup
  weight_decay: 0.01          # L2 regularization
  max_grad_norm: 1.0          # Gradient clipping
  
  # Logging and evaluation (frequent for test)
  logging_steps: 5
  eval_steps: 20
  save_steps: 20  # Must be multiple of eval_steps
  load_best_model_at_end: false  # Don't load best for test
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

# LoRA Configuration
lora:
  enabled: true
  r: 16                       # Rank of adaptation
  lora_alpha: 32              # LoRA scaling parameter
  lora_dropout: 0.1           # Dropout for LoRA layers
  target_modules: ["q", "v"]  # Target attention modules
  bias: "none"                # Bias training strategy
  task_type: "SEQ_2_SEQ_LM"   # Task type for PEFT

# Hardware Configuration
hardware:
  device: "cuda"              # Use CUDA if available
  dataloader_num_workers: 2   # Fewer workers for test
  pin_memory: true            # Pin memory for faster data transfer

# Output Configuration
output:
  root: "reports/test_run"    # Test output directory
  run_name: "flan-t5-base-lora-test"
  report_to: []               # No reporting for test

# Reproducibility
reproducibility:
  seed: 42                    # Random seed
  data_seed: 42               # Data shuffling seed
  deterministic: true         # Deterministic training

# Evaluation Configuration
evaluation:
  max_new_tokens: 128         # Shorter generation for test
  num_beams: 2                # Fewer beams for speed
  length_penalty: 0.6         # Length penalty
  no_repeat_ngram_size: 3     # No repeat n-gram size
  early_stopping: true        # Early stopping in generation
