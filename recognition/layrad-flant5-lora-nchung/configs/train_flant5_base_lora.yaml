# FLAN-T5 Base LoRA Training Configuration
# BioLaySumm Expert-to-Layperson Radiology Report Translation
# Author: Nathan Chung
# Course: COMP3710 Pattern Analysis

# Dataset Configuration
dataset:
  name: "BioLaySumm/BioLaySumm2025-LaymanRRG-opensource-track"
  max_source_length: 512      # Maximum input sequence length (expert reports)
  max_target_length: 256      # Maximum output sequence length (layperson summaries)
  seed: 42                    # Random seed for reproducible shuffling
  local_data_path: null       # Optional local data path override

# Model Configuration
model:
  name: "google/flan-t5-base"  # Base FLAN-T5 model
  torch_dtype: "bfloat16"      # Mixed precision for memory efficiency

# Training Configuration
training:
  batch_size: 8               # Batch size per GPU
  gradient_accumulation_steps: 4  # Effective batch size = 8 * 4 = 32
  learning_rate: 1e-4         # Learning rate for LoRA
  num_epochs: 3               # Number of training epochs
  warmup_steps: 500           # Learning rate warmup steps
  weight_decay: 0.01          # L2 regularization
  max_grad_norm: 1.0          # Gradient clipping
  
  # Early stopping
  early_stopping_patience: 3  # Stop if no improvement for N epochs
  early_stopping_threshold: 0.001  # Minimum improvement threshold
  
  # Mixed precision
  fp16: false                 # Use bfloat16 instead
  bf16: true                  # Better numerical stability than fp16
  
  # Logging and checkpointing
  logging_steps: 100          # Log every N steps
  save_steps: 1000            # Save checkpoint every N steps
  eval_steps: 1000            # Evaluate every N steps
  save_total_limit: 3         # Keep only last N checkpoints

# LoRA Configuration (Parameter-Efficient Fine-Tuning)
lora:
  r: 8                        # LoRA rank (low rank adaptation dimension)
  alpha: 32                   # LoRA scaling parameter (alpha/r = 4.0)
  dropout: 0.1                # LoRA dropout rate to prevent overfitting
  target_modules:             # Modules to apply LoRA to
    - "q"                     # Query projection
    - "v"                     # Value projection
  bias: "none"                # LoRA bias type
  task_type: "SEQ_2_SEQ_LM"   # Sequence-to-sequence language modeling

# Evaluation Configuration
evaluation:
  # Generation parameters for evaluation
  max_new_tokens: 200         # Maximum tokens to generate
  num_beams: 4                # Beam search width
  length_penalty: 0.6         # Length penalty for beam search
  no_repeat_ngram_size: 3     # Prevent repeating n-grams
  early_stopping: true        # Stop generation when EOS token is generated
  
  # Metrics to compute
  metrics:
    - "rouge1"
    - "rouge2" 
    - "rougeL"
    - "rougeLsum"
  
  # Evaluation strategy
  eval_strategy: "steps"      # Evaluate every N steps
  metric_for_best_model: "rougeLsum"  # Best model selection metric
  greater_is_better: true     # Higher ROUGE scores are better

# Hardware Configuration
hardware:
  device: "cuda"              # Device to use (cuda/cpu)
  dataloader_num_workers: 4   # Number of data loading workers
  pin_memory: true            # Pin memory for faster GPU transfer
  
# Distributed Training (for multi-GPU)
distributed:
  use_torchrun: false         # Use torchrun for distributed training
  num_processes: 1            # Number of processes (GPUs)
  backend: "nccl"             # Distributed backend

# Output Configuration
output:
  output_dir: "./checkpoints/flan-t5-base-lora-biolaysumm"
  run_name: "flan-t5-base-lora-biolaysumm"
  report_to: ["tensorboard"]  # Logging backends
  hub_model_id: null          # HuggingFace Hub model ID (if pushing)

# Reproducibility
reproducibility:
  seed: 42                    # Global random seed
  data_seed: 42               # Data shuffling seed
  model_seed: 42              # Model initialization seed
  set_seed: true              # Set all random seeds

# Data Processing
data_processing:
  remove_unused_columns: true # Remove unused columns after tokenization
  load_from_cache_file: false # Always reprocess data for consistency
  preprocessing_num_workers: 1 # Number of workers for preprocessing
